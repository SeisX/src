Regularized linear least squares estimation
problems can be written as minimizing the quadratic
function
\beq
Q(m) = \|\data- \bf L \bf m\|^2 + \epsilon^2 \|\bf A \bf m\|^2
\eeq
where $\data$ is our data, $\bf L$ is our modeling operator,
$\reg$ is our regularization operator and we are inverting
for  a model $\bf m$.
Alternately, we can write  them
in terms of  fitting goals,
\beqa
\data &\approx& \bf L \bf m 
\label{eq:geophysics} \\
\nonumber
\zero &\approx& \epsilon  \reg \bf m 
,
\eeqa
For the purpose of this paper I will refer to the first
goal as the {\it data fitting goal} and the second as
the {\it model styling goal}.
Normally we think of {\it data fitting goal } as describing
the physics of the problem.  The {\it model styling goal} is suppose
to provide information about the model character.  Ideally 
$\reg$ should be the inverse model covariance.   In practice
we don't have the model covariance so we attempt  to approximate it through
another operator.   At SEP the regularization operator is  typically one
of the following:
\begin{description}
\item [Laplacian or gradient]  a simple operator that assumes nothing about the model
\item [Prediction Error Filter (PEF)]  a stationary operator  estimated from known portions of the model or  some field with the same properties as the model \cite[]{gee}
\item [steering filter] a non-stationary operator built from minimal information about the model \cite[]{Clapp.sep.95.bob1}
\item [non-stationary PEF] a non-stationary operator built from a field with the same properties as the model \cite[]{Crawley.sep.104}.
\end{description}
A problem with the first three operators is
that while they approximate the model covariance, they
have little concept of model variance.  As a result our model estimates  tend
to have the wrong statistical properties.

